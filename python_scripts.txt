If error "System Error: Cannot compile 'Python.h'. Perhaps you need to install python-dev|python-devel" when you try to sudo pip install numpy, you will need to install python-dev first
  sudo apt-get install python-dev

To install pip install scipy
  sudo apt-get install gfortran libopenblas-dev liblapack-dev
  sudo pip install scipy
  sudo pip install scikit-learn
  
To install pip install matplotlib:
  sudo apt-get install libfreetype6
  sudo apt-get install libxft-dev
  sudo pip install matplotlib
  
To allow Python interpreter to recognize Spark in Ubuntu
  Download Spark from Spark Website (Spark 2.1.0 Hadoop 2.6)
  cd <LOCATION_WHERE_YOU_WANT_SPARK_TO_BE_LOCATED>
  tar xzvf spark-2.1.0-bin-hadoop2.6.tgz
  sudo vi ~/.bashrc
  add the following lines:
    export SPARK_HOME='<LOCATION_WHERE_YOU_WANT_SPARK_TO_BE_LOCATED>/spark-2.1.0-bin-hadoop2.6'
    export PATH=$SPARK_HOME:$PATH
    export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH
  sudo pip install py4j
  Launch a new Terminal, and test the installation
    >>>python
    >>>from pyspark.sql import SparkSession
  
To perform unit test cases on Python scripts
    sudo pip install nose
    sudo pip install coverage
    Open terminal, nosetests -s
    It will generate a cover folder
    To run nose for a single unit test case, append @attr('now') before the function
    Then issue the command: nosetests -s -a now

To run nose in Eclipse PyDev
     Right click the <test-case-file>.py --> Run As --> Run Configuration...
      Select the <test-case-file>.py
      Click Arguments tab
      Under PyUnit section --> check "Override PyUnt preferences for this launch
      Select Nose test Runner
      Enter -s -a now in the textbox and click Run button. It will only run test cases with @attr('now') append to it

To add PySpark to PyCharm IDE
  In an existing project, File --> Settings
  Under Project: <YOUR_PROJECT_NAME> --> Project Structure
  Click "Add content root"
  Navigate to spark-2.1.0-bin-hadoop2.6/python/lib and add py4j-0.10.3-src.zip & pyspark.zip
  Add variables PYHTONPATH, SPARK_HOME
    Run --> Edit Configurations
    Under Environment --> Environment Variables
    Create two new variables: 
      PYHTONPATH  <LOCATION_WHERE_YOU_WANT_SPARK_TO_BE_LOCATED>/spark-2.0.2-bin-hadoop2.6/python/
      SPARK_HOME  <LOCATION_WHERE_YOU_WANT_SPARK_TO_BE_LOCATED>/spark-2.0.2-bin-hadoop2.6
  To run a single test case only:
    Run --> Edit Configurations
    Check Params
    Enter -s -a now
    In the py script add @attr('now') to run that specific test case
    
Pyspark codes:
---------------
A. Codes to create a spark context & sqlContext
    from pyspark.sql import SparkSession
    from pyspark.sql import SQLContext, Row
    
    spark = SparkSession.builder.appName("Python Spark SQL basic example").config("spark.some.config.option", "some-value").getOrCreate()
    sc = spark.sparkContext
    sc.addPyFile("/home/Documents/Project/src/WebConnectLocalSpark.py") #this is required if you are running another program calling this script for pyspark process or the caller complain of "ImportError: No module named WebConnectLocalSpark"
    sqlContext = SQLContext(sc)
    
B. Load parquet files from local
    parquetFileSelected = sqlContext.read.parquet("/home/Documents/Project/resources/selected/*.snappy.parquet");

C. Manipulating spark dataframe
    X = parquetFileSelected.select('features') #select only the feeatures column
    X = X.rdd.map(lambda x: np.array(x)[0])
    X = X.collect() #convert to a list
    
    total_no_of_clusters = parquetFileSelected.select("storyid").distinct().count() #count the distinct number of items in the storyid column
    
    counts_in_a_cluster = parquetFileSelected.groupby('storyid') #group all the records by storyid
    counts_in_a_cluster = sorted(counts_in_a_cluster.agg({"*": "count"}).collect()) #for each storyid group, count the number of records and sort them in ascending order. Return as a list
    hashtable_count = dict(counts_in_a_cluster)#convert list into a dictionary (hashtable)
    
Numpy codes:
-------------
import numpy as np

labels_array = np.asarray(labels) #convert list to numpy array
samples_result_merge = np.column_stack((labels_array, samples_result))#merge labels_array and samples_result to form 2D array

Pandas codes:
-------------
import pandas as pd

samples_result_pandas = pd.DataFrame(samples_result_merge) #convert numpy array to pandas
samples_result_pandas = samples_result_pandas.rename(columns={0: 'label', 1: 'silh'})#rename columns using index
samples_result_pandas['silh'] = samples_result_pandas['silh'].astype(float) #convert the silhouette column to float
filter_result_1 = samples_result_pandas[(samples_result_pandas['silh'] > 0.5)]  # filter out those which meet the silh value threshold
filter_result_1 = samples_result_pandas.groupby('label').size()  # group by label and calculate the size of each group
hashtable_silh_0_5 = filter_result_1.to_dict()#convert to a dictionary (hashtable)

    
