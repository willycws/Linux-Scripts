If error "System Error: Cannot compile 'Python.h'. Perhaps you need to install python-dev|python-devel" when you try to sudo pip install numpy, you will need to install python-dev first
  sudo apt-get install python-dev

To install pip install scipy
  sudo apt-get install gfortran libopenblas-dev liblapack-dev
  sudo pip install scipy
  sudo pip install scikit-learn
  
To install pip install matplotlib:
  sudo apt-get install libfreetype6
  sudo apt-get install libxft-dev
  sudo pip install matplotlib
  
To allow Python interpreter to recognize Spark in Ubuntu
  Download Spark from Spark Website (Spark 2.1.0 Hadoop 2.6)
  cd <LOCATION_WHERE_YOU_WANT_SPARK_TO_BE_LOCATED>
  tar xzvf spark-2.1.0-bin-hadoop2.6.tgz
  sudo vi ~/.bashrc
  add the following lines:
    export SPARK_HOME='<LOCATION_WHERE_YOU_WANT_SPARK_TO_BE_LOCATED>/spark-2.1.0-bin-hadoop2.6'
    export PATH=$SPARK_HOME:$PATH
    export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH
  sudo pip install py4j
  Launch a new Terminal, and test the installation
    >>>python
    >>>from pyspark.sql import SparkSession
  
To perform unit test cases on Python scripts
    sudo pip install nose
    sudo pip install coverage
    Open terminal, nosetests -s
    It will generate a cover folder
    To run nose for a single unit test case, append @attr('now') before the function
    Then issue the command: nosetests -s -a now

To run nose in Eclipse PyDev
     Right click the <test-case-file>.py --> Run As --> Run Configuration...
      Select the <test-case-file>.py
      Click Arguments tab
      Under PyUnit section --> check "Override PyUnt preferences for this launch
      Select Nose test Runner
      Enter -s -a now in the textbox and click Run button. It will only run test cases with @attr('now') append to it

To add PySpark to PyCharm IDE
  In an existing project, File --> Settings
  Under Project: <YOUR_PROJECT_NAME> --> Project Structure
  Click "Add content root"
  Navigate to spark-2.1.0-bin-hadoop2.6/python/lib and add py4j-0.10.3-src.zip & pyspark.zip
  Add variables PYHTONPATH, SPARK_HOME
    Run --> Edit Configurations
    Under Environment --> Environment Variables
    Create two new variables: 
      PYHTONPATH  <LOCATION_WHERE_YOU_WANT_SPARK_TO_BE_LOCATED>/spark-2.0.2-bin-hadoop2.6/python/
      SPARK_HOME  <LOCATION_WHERE_YOU_WANT_SPARK_TO_BE_LOCATED>/spark-2.0.2-bin-hadoop2.6
  To run a single test case only:
    Run --> Edit Configurations
    Check Params
    Enter -s -a now
    In the py script add @attr('now') to run that specific test case
    
Pyspark codes:
---------------
A. Codes to create a spark context & sqlContext
    from pyspark.sql import SparkSession
    from pyspark.sql import SQLContext, Row
    
    spark = SparkSession.builder.appName("Python Spark SQL basic example").config("spark.some.config.option", "some-value").getOrCreate()
    sc = spark.sparkContext
    sc.addPyFile("/home/Documents/Project/src/WebConnectLocalSpark.py") #this is required if you are running another program calling this script for pyspark process or the caller complain of "ImportError: No module named WebConnectLocalSpark"
    sqlContext = SQLContext(sc)
    
B. Load parquet files from local
    parquetFileSelected = sqlContext.read.parquet("/home/Documents/Project/resources/selected/*.snappy.parquet");
